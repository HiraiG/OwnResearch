{"cells":[{"cell_type":"markdown","metadata":{"id":"mBOw_UkilzsH"},"source":["# 2023年 世界モデル 第2回演習"]},{"cell_type":"markdown","metadata":{"id":"F5qkQczU7d0d"},"source":["この演習では，**ベルヌーイ分布，混合ベルヌーイ分布，混合ガウス分布**を用いてMNISTデータセットをモデリングしていきます．"]},{"cell_type":"markdown","metadata":{"id":"VjSv9Df6ZuJN"},"source":["## 目次\n","\n","1. [ベルヌーイ分布によるMNIST画像モデリング](#scrollTo=p-KulWxYNbXo)\n","1. [EMアルゴリズムによる混合ベルヌーイ分布の最適化](#scrollTo=YM49msUsKbkw)\n","1. [EMアルゴリズムによる混合ガウス分布の最適化](#scrollTo=57Tdk0SEPqQ2)\n","1. [参考文献](#scrollTo=TWo05r1KEUiR&line=1&uniqifier=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GgdvzEkT_qRT"},"outputs":[],"source":["# 必要なライブラリのインポート\n","\n","import copy\n","import math\n","import random\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import seaborn as sns\n","from keras.datasets import mnist\n","from scipy import stats\n","\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1KPyJdXv1KIs"},"outputs":[],"source":["def set_seed(seed: int = 1234):\n","    \"\"\"\n","    シード値を設定する関数．\n","    ここではrandomとNumPyのみですが，他にシード値固定が必要なライブラリがあれば追加する必要があります．\n","\n","    Parameters\n","    ----------\n","    seed : int\n","        設定するシード値．\n","\n","    Returns\n","    -------\n","    None\n","    \"\"\"\n","    random.seed(seed)\n","    np.random.seed(seed)\n","\n","\n","set_seed(0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cONqGi_yMdPF"},"outputs":[],"source":["# ヘルパー関数の定義\n","\n","\n","def get_label_idxs(labels: list, t_mnist: np.ndarray) -> np.ndarray:\n","    \"\"\"\n","    MNISTから利用したい数字に対応するデータのindexを取得する関数．\n","\n","    Parameters\n","    ----------\n","    labels : list\n","        利用したい数字をまとめたリスト．要素は0-9をとることを想定している．\n","    t_mnist : np.ndarray (batch_size, )\n","        MNSITの教師データ．\n","\n","    Returns\n","    -------\n","    label_ids : np.ndarray (利用するデータの数, )\n","        リスト内に含まれる数字が正解であるデータのindexをまとめた配列．\n","    \"\"\"\n","    # 長さ60000のt_mnistについて，値が指定したラベルのうちのいずれかであるかのboolean arrayを取ってくる\n","    label_bool = np.any([t_mnist == label for label in labels], axis=0)\n","    # Trueである要素のインデックスを得る\n","    label_idxs = np.where(label_bool)[0]\n","\n","    return label_idxs\n","\n","\n","def transforms(data_all: np.ndarray, flatten=True, binarize=True) -> np.ndarray:\n","    \"\"\"\n","    画像データを1次元配列に平坦化し，画素値を{0, 1}に二値化する関数．\n","\n","    Parameters\n","    ----------\n","    data_all : np.ndarray (batch_size, height, width)\n","        MNISTの入力データ．\n","    flatten : bool\n","        入力データを1次元配列にするかどうか判別するフラグ．\n","        Falseの場合は2次元のまま．\n","    binarize : bool\n","        入力データを二値化するかどうか判別するフラグ．\n","        Falseの場合は[0, 1]に正規化される．\n","\n","    Returns\n","    -------\n","    data_all : np.ndarray (batch_size, height x width)\n","        平坦化，画素値の二値化を行った結果のデータ．\n","        flatten=Falseの場合は配列の形が(batch_size, height, width)になる．\n","        binarize=Falseの場合は画素値が[0, 1]正規化された連続値を取る．\n","    \"\"\"\n","    # 範囲を0~255から0~1にし，平坦化したあと，閾値0.5で0,1のバイナリにする\n","    data_all = data_all.astype(np.float64) / 255\n","    if flatten:\n","        data_all = data_all.reshape((data_all.shape[0], -1))\n","    if binarize:\n","        data_all = (data_all > 0.5).astype(np.uint8)\n","\n","    return data_all"]},{"cell_type":"markdown","metadata":{"id":"p-KulWxYNbXo"},"source":["## 1. ベルヌーイ分布によるMNIST画像モデリング"]},{"cell_type":"markdown","metadata":{"id":"XzTJ8EcKN93g"},"source":["本セクションでは，MNIST画像の1種類のラベルのみを，単純な（多次元）ベルヌーイ分布でモデリングしてみます．\n","\n","そのために，ここではラベルが1のサンプルのみを600枚取ってきます．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0NJVQ4TQTZFW"},"outputs":[],"source":["NUM_SAMPLES = 600\n","\n","(x_mnist, t_mnist), _ = mnist.load_data()\n","print(f\"x_mnist: {x_mnist.shape}, t_mnist: {t_mnist.shape}\")\n","\n","labels = [1]\n","\n","label_idxs = get_label_idxs(labels, t_mnist)\n","\n","data_all = x_mnist[label_idxs][:NUM_SAMPLES]\n","print(f\"data_all: {data_all.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"qgM_QmalV2yp"},"source":["最初のサンプルをプロットしてみます．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G9o-FD9hTukL"},"outputs":[],"source":["plt.figure(figsize=(10, 10))\n","sns.heatmap(\n","    data_all[0],\n","    cmap=\"gray\",\n","    annot=True,\n","    fmt=\"d\",\n","    annot_kws={\"fontsize\": 8},\n","    square=True,\n",")"]},{"cell_type":"markdown","metadata":{"id":"uC-IaOqFXheQ"},"source":["多次元ベルヌーイ分布によるモデリングを行うため，ピクセル値の範囲を0-255から0-1にし，{0,1}のバイナリにしてから，データを平坦化します．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z-r0A7RPRR0G"},"outputs":[],"source":["binary_data = transforms(data_all)\n","print(f\"binary_data: {binary_data.shape}, set {np.unique(binary_data)}\")"]},{"cell_type":"markdown","metadata":{"id":"1x2zT6XUSdkL"},"source":["あるピクセルについて，値の分布をプロットしてみます．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6eNdLW93dbQX"},"outputs":[],"source":["TARGET_PIXEL = 157\n","\n","binary_pixel = binary_data[:, TARGET_PIXEL]  # ( 600, )\n","\n","print(\"0の回数: \", len(binary_pixel) - sum(binary_pixel))\n","print(\"1の回数: \", sum(binary_pixel))\n","\n","plt.hist(binary_pixel)\n","plt.xlabel(\"Binarized pixel values\")\n","plt.ylabel(\"Sample count\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"jLnpahn6hd8W"},"source":["各ピクセルそれぞれに個別のパラメータを持ったベルヌーイ分布が存在すると考えるので，ここでは一つのピクセルについてのみ記述します（つまり$D=784$のインデックスを表す$d$を省略）．\n","\n","観測データ集合を$X=\\left\\{x_{i}\\right\\}_{i=1}^{N}$とします($N=600$)．$x_{i}$は$i$枚目の画像のあるピクセルの値を，閾値0.5で0，1に2値化したものです．\n","\n","生成モデルの分布としてパラメータが$\\mu$のベルヌーイ分布を考え，モデルを設計します．\n","\n","$$\n","p_{\\mu}({\\bf x})=\\prod_{i=1}^{N} \\mu^{x_{i}}(1-\\mu)^{1-x_{i}}\n","$$\n","\n","$\\mu$の推定量$\\hat{\\mu}$を最尤推定で求めます．\n","\n","$$\n","\\hat{\\mu}=\\underset{\\mu}{\\operatorname{argmax}} \\sum_{i=1}^{N}\\left[x_{i} \\log \\mu+\\left(1-x_{i}\\right) \\log (1-\\mu)\\right]\n","$$\n","\n","対数尤度関数を$\\mu$について偏微分してゼロとおくことで，最尤推定量は次のように求まります．\n","\n","$$\n","\\hat{\\mu}=\\frac{1}{N}\\sum_{i=1}^{N} x_{i}\n","$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eZSLir9umgks"},"outputs":[],"source":["mu_hats = np.mean(binary_data, axis=0)  # ( 784, )\n","print(f\"mu_hats: {mu_hats.shape}\")\n","\n","# TARGET_PIXELのmu_hatをパラメータとするベルヌーイ分布からデータを生成する\n","sampled_data = stats.bernoulli.rvs(p=mu_hats[TARGET_PIXEL], size=NUM_SAMPLES)\n","print(\"0の回数: \", len(sampled_data) - sum(sampled_data))\n","print(\"1の回数: \", sum(sampled_data))\n","\n","plt.hist(sampled_data)\n","plt.xlabel(\"Binarized pixel values\")\n","plt.ylabel(\"Sample count\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"UnmS6icqWLoX"},"source":["ベルヌーイ分布の$\\hat{\\mu}$を直接可視化してみます．ある程度1を生成するようなパラメータ集合が得られていることがわかります．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3gcwabjCWxcx"},"outputs":[],"source":["plt.imshow(mu_hats.reshape(28, 28), cmap=\"gray\")"]},{"cell_type":"markdown","metadata":{"id":"40Nykl8XXTfC"},"source":["次に，ラベルが0のサンプルを混ぜて，同じ多次元ベルヌーイ分布でモデリングを行ってみます．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HPDJII4qXiCp"},"outputs":[],"source":["NUM_SAMPLES = 1200\n","\n","labels = [0, 1]\n","\n","label_idxs = get_label_idxs(labels, t_mnist)\n","data_all = x_mnist[label_idxs][:NUM_SAMPLES]\n","print(f\"data_all: {data_all.shape}\")\n","\n","binary_data = transforms(data_all)\n","print(f\"binary_data: {binary_data.shape}, set {np.unique(binary_data)}\")\n","\n","# 最尤推定\n","mu_hats = np.mean(binary_data, axis=0)  # ( 784, )\n","print(f\"mu_hats: {mu_hats.shape}\")\n","\n","plt.imshow(mu_hats.reshape(28, 28), cmap=\"gray\")"]},{"cell_type":"markdown","metadata":{"id":"QR8vcFT7YMs-"},"source":["0とも1とも言えない画像を生成するようなパラメータが得られてしまいました．\n","\n","データのソースに明らかに複数の確率分布が考えられる場合は，混合分布が有用です．\n","\n","しかし，講義でもあったように，混合分布では対数尤度関数でlogの中に和の形があるため，上記のような閉形式で（微分してゼロとおくようなアプローチで）最尤推定を行うことができません．\n","\n","そこで次に，**EMアルゴリズム**[[1]](#scrollTo=TWo05r1KEUiR&line=2&uniqifier=1)によって混合ベルヌーイ分布を最適化することを考えていきます．"]},{"cell_type":"markdown","metadata":{"id":"YM49msUsKbkw"},"source":["## 2. EMアルゴリズムによる混合ベルヌーイ分布の最適化"]},{"cell_type":"markdown","metadata":{"id":"K2ckryGzL1ro"},"source":["### 2.1 問題設定\n","\n","本セクションでは，混合ベルヌーイ分布を用いてMNISTデータセットをモデリングしていきます．\n","\n","以下では，ラベルが **0, 5, 7** の画像を合わせて1800枚用います．"]},{"cell_type":"markdown","metadata":{"id":"exzgW62piNqp"},"source":["観測データ集合は${\\bf X}=\\left\\{{\\bf x}_{i}\\right\\}_{i=1}^{N}$（ただし$N=1800$）．${\\bf x}_i$は2値化された$i$枚目の画像（長さ784のベクター）です．\n","\n","$K(=3)$個のラベルのうち，どのラベルの画像の値かを示す**潜在変数**を$\\mathbf{z}=\\left[z_1, z_2, z_3\\right]^{T} \\in\\{0,1\\}^3$とします．\n","\n","- つまり$\\mathbf{z}$は長さ$K$のone-hotベクトルで，${\\sum_{k=1}^{K} z_{k}=1},\\quad z_k \\in \\{0,1\\}$\n","\n","- $\\mathbf{z} = [1, 0, 0]$ならラベルが0の画像を表す\n","\n","$\\mathbf{z}$が従う確率分布をカテゴリ分布とし，$z_k = 1$となる確率を$\\pi_{k}$とします．\n","\n","- $\\sum_{k=1}^{K}\\pi_{k}=1\\qquad \\pi_{k} \\in[0,1]$\n","\n","- カテゴリ分布の確率質量関数　$p_{\\pi}(\\mathbf{z})=\\prod_{k=1}^{K} \\pi_{k}^{z_{k}}$"]},{"cell_type":"markdown","metadata":{"id":"_gnOogTu2oeO"},"source":["### 2.2 混合ベルヌーイ分布の復習\n","\n","- 講義スライドの「潜在変数モデルと混合モデル」を参照してください．\n","\n","ある潜在変数${\\bf z}$を選んだ上で，確率変数$x$が従う確率分布は，\n","\n","$$\n","p_{\\mu}(x|{\\bf z}) = \\prod_{k=1}^K\\left\\{ \\mu_k^x(1-\\mu_k)^{1-x}\\right\\}^{z_k}\n","$$\n","\n","- $z_k=1$となる$k$以外は$z_k=0$となるので，積に影響しない．\n","\n","同時分布\n","\n","$$\n","p_{\\mu,\\pi}(x,{\\bf z}) = p_{\\mu}(x|{\\bf z})p_{\\pi}({\\bf z})\n","= \\prod_{k=1}^K \\left\\{ \\pi_k \\mu_k^x (1-\\mu_k)^{1-x} \\right\\} ^{z_k}\n","$$\n","\n","周辺分布\n","\n","$$\n","p_{\\mu,\\pi}(x) = \\sum_z p_{\\mu,\\pi}(x,{\\bf z})\n","= \\sum_{k=1}^K p_{\\mu,\\pi}(x,z_k=1)\n","= \\sum_{k=1}^K\\pi_k\\mu_k^x (1-\\mu_k)^{1-x}\n","$$\n","\n","$x$の周辺分布は，ベルヌーイ分布の線形和で表されることがわかりました．\n","\n","この対数周辺尤度を最大化するようなパラメータ${\\mu_k},{\\pi_k}$を求めるために，EMアルゴリズムを用いて最適化を行っていきます．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5aFr4snQOamI"},"outputs":[],"source":["NUM_SAMPLES = 1800\n","\n","(x_mnist, t_mnist), _ = mnist.load_data()\n","print(f\"x_mnist: {x_mnist.shape}, t_mnist: {t_mnist.shape}\")\n","\n","labels = [0, 5, 7]\n","\n","label_idxs = get_label_idxs(labels, t_mnist)\n","\n","data_all = x_mnist[label_idxs][:NUM_SAMPLES]\n","print(f\"data_all: {data_all.shape}\")\n","\n","# 範囲を0~255から0~1にし，平坦化したあと，閾値0.5で0,1のバイナリにする\n","binary_data = transforms(data_all)\n","print(f\"binary_data: {binary_data.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"lhoayQJHcliY"},"source":["#### 多次元ベルヌーイ分布の対数尤度\n","\n","1枚の画像について，多次元ベルヌーイ分布の確率質量関数は以下のようになります．$D$はピクセル数で，MNISTの場合は784です．（$d$の添字が$\\mu$にもついていることに注意してください．）\n","\n","$$\n","p_\\mu({\\bf x}) = \\prod_{d=1}^D\\mu_d^{x_d}(1-\\mu_d)^{1-{x_d}}\n","$$\n","\n","対数をとると，\n","\n","$$\n","{\\rm log}\\ p_\\mu({\\bf x}) = \\sum_{d=1}^D \\left\\{x_d\\ {\\rm log\\ }\\mu_d + (1 - x_d)\\ {\\rm log\\ }(1 - \\mu_d)\\right\\}\n","$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NL5u-ffHYUWe"},"outputs":[],"source":["# float型では、（大体）1e-323未満の値は精度の問題で0.0となり、\n","# 対数を取ると -inf となるので下限を抑えておく\n","def np_log(x: np.ndarray) -> np.ndarray:\n","    \"\"\"\n","    配列内の要素が-infにならないように下限を設定する．\n","\n","    Parameters\n","    ----------\n","    x : np.ndarray\n","        要素をクリッピングする対象の配列．形状に指定はない．\n","\n","    Returns\n","    -------\n","    x : np.ndarray\n","        要素をクリッピングしたあとの配列．最小値が1e-323，最大値が1e+10にされる．\n","    \"\"\"\n","    return np.log(np.clip(a=x, a_min=1e-323, a_max=1e10))\n","\n","\n","# ベルヌーイ分布の対数尤度を計算する関数\n","def log_bernoulli_density(x: np.ndarray, mu: np.ndarray) -> float:\n","    \"\"\"\n","    ベルヌーイ分布の対数尤度を計算する関数．\n","    各ピクセルに対するベルヌーイ分布の対数尤度を計算した総和を取る．\n","\n","    Parameters\n","    ----------\n","    x : np.ndarray (784, )\n","        MNISTの入力データ．平坦化されていることを想定している．\n","    mu : np.ndarray (784, )\n","        各ピクセルに対するベルヌーイ分布のパラメータ（平均値）．\n","\n","    Returns\n","    -------\n","    log_p : float\n","        各ピクセルに対するベルヌーイ分布の対数尤度の総和．\n","    \"\"\"\n","    log_p = x * np_log(mu) + (1 - x) * np_log(1 - mu)  # ( 784, )\n","    log_p = np.sum(log_p)\n","    return log_p"]},{"cell_type":"markdown","metadata":{"id":"6WNopwbih3RE"},"source":["### 2.3 全データに対する，混合ベルヌーイ分布の対数尤度\n","\n","データ（画像）の数を$N$，混合数を$K$，各ベルヌーイ分布を$p_{\\mu_k}(x)$とすると，全データ点に対する混合ベルヌーイ分布の対数尤度は，以下で表されます．\n","\n","$$\n","{\\rm log}\\  p_\\mu(X)= \\sum_{i=1}^N{\\rm log}\\sum_{k=1}^K\\pi_k p_{\\mu_k}({\\bf x}_i)\n","$$\n","<!--\\quad\\left( = \\sum_{i=1}^N\\log\\sum_{k=1}^K\\pi_k\\mu_k^{x_i}(1-\\mu_k)^{1-x_i} \\right) -->"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sdfHOg4qhu24"},"outputs":[],"source":["def log_likelihood(\n","    binary_data: np.ndarray, mu_list: np.ndarray, pi_list: np.ndarray\n",") -> float:\n","    \"\"\"\n","    全てのデータ点を用いた，混合ベルヌーイ分布に対する対数尤度を計算する関数．\n","\n","    Parameters\n","    ----------\n","    binary_data : np.ndarray (NUM_SAMPLES, 784)\n","        すべてのデータ点をまとめた配列．\n","    mu_list : list [ (784), (784), (784) ]\n","        各ピクセルのベルヌーイ分布のパラメータ（平均値）．\n","    pi_list : np.ndarray (3, )\n","        混合率をまとめた配列．\n","\n","    Returns\n","    -------\n","    log_likely_all : float\n","        混合ベルヌーイ分布に対する対数尤度．\n","    \"\"\"\n","    # binary_data: ( 1800, 784 ), mu_list: [ (784), (784), (784) ], pi_list: ( 3, )\n","    log_likely_all = 0\n","    for data in binary_data:\n","        likely = 0\n","        for k in range(len(pi_list)):\n","            likely += pi_list[k] * np.exp(log_bernoulli_density(data, mu_list[k]))\n","        log_likely_all += np_log(likely)\n","\n","    return log_likely_all"]},{"cell_type":"markdown","metadata":{"id":"GSh-WJQKiyAs"},"source":["### 2.4 負担率\n","\n","負担率は，各データ点が与えられた上での潜在変数（カテゴリ）の事後確率を指します．\n","\n","負担率を計算することは，各データ点がどのカテゴリからサンプリングされたかを推論することに相当します．\n","\n","任意のピクセルに対し，$i$番目のデータ（画像）に対する，$k$番目の潜在変数を$z_{ik}$とすると，負担率は以下で表されます．\n","\n","<!--$$\n","p_{\\mu,\\pi}(z_{ik}=1 | {\\bf x}_i)\n","= \\frac{p_{{\\mu_k},{\\pi_k}}({\\bf x}_i,z_{ik}=1)}{\\sum_{j=1}^K p_{{\\mu_j},{\\pi_j}}({\\bf x}_i, z_{ik}=1)}\n","=\\frac{\\pi_k \\mu_k^{{\\bf x}_i}(1-\\mu_k)^{1-{\\bf x}_i}}{\\sum_{j=1}^K\\pi_j \\mu_j^{{\\bf x}_i}(1-\\mu_j)^{1-{\\bf x}_i}}\n","\\equiv \\gamma(z_{ik})\n","$$-->\n","\n","$$\n","p_{\\mu,\\pi}(z_{ik}=1 | {\\bf x}_i)\n","= \\frac{p_{{\\mu},{\\pi}}({\\bf x}_i,z_{ik}=1)}{\\sum_{z} p_{\\mu,\\pi}({\\bf x}_i, z_{ik}=1)}\n","=\\frac{\\pi_k\\ p_{\\mu_k}({\\bf x}_i)}{\\sum_{j=1}^K\\pi_j\\ p_{\\mu_j}({\\bf x}_i)}\n","\\equiv \\gamma(z_{ik})\n","$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jvDSYTboiyLC"},"outputs":[],"source":["def responsibility(data_all: np.ndarray, mu_list: list, pi_list: list) -> np.ndarray:\n","    \"\"\"\n","    負担率を計算する関数．\n","\n","    Parameters\n","    ----------\n","    binary_data : np.ndarray (NUM_SAMPLES, 784)\n","        すべてのデータ点をまとめた配列．\n","    mu_list : list [ (784), (784), (784) ]\n","        各ピクセルのベルヌーイ分布のパラメータ（平均値）．\n","    pi_list : list (3, )\n","        混合率をまとめた配列．\n","\n","    Returns\n","    -------\n","    gamma_mat : np.ndarray\n","        各データに対する負担率．\n","    \"\"\"\n","    gamma_mat = np.zeros((data_all.shape[0], len(pi_list)))  # ( 1800, 3 )\n","    for i in range(gamma_mat.shape[0]):\n","        for k in range(gamma_mat.shape[1]):\n","            gamma_mat[i][k] = log_bernoulli_density(data_all[i], mu_list[k])\n","        gamma_mat[i] -= np.max(gamma_mat[i])\n","        gamma_mat[i] = pi_list * np.exp(gamma_mat[i])\n","        gamma_mat[i] /= np.sum(gamma_mat[i])\n","    return gamma_mat"]},{"cell_type":"markdown","metadata":{"id":"r71jfdoDi9MW"},"source":["### 2.5 学習\n","\n","EMアルゴリズムによる学習は，EステップとMステップを繰り返すことで最適化を行います．\n","\n","1. Eステップ  \n","現在のパラメータのもとで，事後分布（負担率）を計算する．　$\\gamma(z_{ik})=p_{\\mu,\\pi}(z_{ik}=1|{\\bf x}_i)$\n","\n","1. Mステップ  \n","負担率を用いて，新しいパラメータ$\\mu_k$と$\\pi_k$を推定して更新する．\n","\n","混合ベルヌーイ分布ではMステップにおける更新式は以下となります(証明等はテキストを参照してください)\n","\n","$$\n","\\hat\\pi_k = \\frac{N_k}{N}\\qquad\n","\\hat\\mu_k = \\frac{1}{N_k} \\sum_{i=1}^N\\gamma(z_{ik})x_i\n","\\qquad ただし，N_k = \\sum_{i=1}^N\\gamma(z_{ik})\n","$$\n","\n","$\\hat\\pi_k$は全データの負担率の平均値，$\\hat\\mu_k$は負担率による重み付きサンプル平均に相当します．\n","\n","EMアルゴリズムでは，各ステップごとの対数尤度が非減少であることが保証されており，増加幅が一定値以下となったら収束したとみなして，学習を終了します．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QkGY4ITUkAIc"},"outputs":[],"source":["# 各ベルヌーイ分布のパラメータを適当な値で初期化\n","mu_list = [np.random.rand(binary_data.shape[1]) for _ in labels]  # それぞれ ( 784, )\n","\n","# 混合分布の重みを初期化\n","pi_list = [0.3, 0.3, 0.4]\n","\n","n_iter = 0\n","\n","# 現在のパラメータを用いて全データに対する対数尤度を計算\n","likely = log_likelihood(binary_data, mu_list, pi_list) / binary_data.shape[0]\n","print(\"Iteration: {0}, log_likelihood: {1}\".format(n_iter, likely))\n","\n","fig = plt.figure()\n","for i in range(len(mu_list)):\n","    ax = fig.add_subplot(1, len(mu_list), i + 1, xticks=[], yticks=[])\n","    ax.imshow(mu_list[i].reshape(28, 28), \"gray\")\n","plt.show()\n","\n","# 対数尤度の上がり幅がth以下になったら収束したと判定する\n","th = 0.001\n","\n","# 学習\n","while True:\n","    n_iter += 1\n","\n","    # Eステップ：現在のパラメータのもとでの事後分布（負担率）を計算\n","    gamma_mat = # WRITE ME  # ( 1800, 3 )\n","    n_k = np.sum(gamma_mat, axis=0)  # ( 3, )\n","\n","    # Mステップ：負担率を用いて，新しいパラメータを推定・更新\n","    # piの新しい推定量\n","    pi_list_next = # WRITE ME  # .tolist()\n","\n","    # gamma_mat: ( 1800, 3 ) binary_data: ( 1800, 784 ) n_k: ( 3, )\n","    mu_list_next = # WRITE ME  # ( 3, 784 )\n","    # muの新しい推定量．0次元目だけリストになる\n","    mu_list_next = list(mu_list_next)  # [( 784, ), ( 784, ), ( 784, )]\n","\n","    mu_list = copy.deepcopy(mu_list_next)\n","    pi_list = copy.deepcopy(pi_list_next)\n","\n","    likely_before = likely\n","\n","    likely = log_likelihood(binary_data, mu_list, pi_list) / binary_data.shape[0]\n","\n","    print(\"Iteration: {0}, log_likelihood: {1}\".format(n_iter, likely))\n","\n","    delta = likely - likely_before\n","\n","    print(\"pi_list: {}\".format(pi_list))\n","    # print(\"mu_list avg: {}\".format([mu.mean() for mu in mu_list]))\n","\n","    fig = plt.figure()\n","    for i in range(len(mu_list)):\n","        ax = fig.add_subplot(1, len(mu_list), i + 1, xticks=[], yticks=[])\n","        ax.imshow(mu_list[i].reshape(28, 28), \"gray\")\n","    plt.show()\n","\n","    if delta < th and n_iter > 20:\n","        break"]},{"cell_type":"markdown","metadata":{"id":"UIptGeqo-a03"},"source":["最後に，最適化したパラメータのベルヌーイ分布からサンプリングを行ってみます．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yI1JF24r_1jU"},"outputs":[],"source":["sampled_data = [np.random.binomial(n=1, p=mu) for mu in mu_list]\n","\n","fig = plt.figure()\n","for i, sample in enumerate(sampled_data):\n","    ax = fig.add_subplot(1, len(mu_list), i + 1, xticks=[], yticks=[])\n","    ax.imshow(sample.reshape(28, 28), \"gray\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"57Tdk0SEPqQ2"},"source":["## 3. EMアルゴリズムによる混合ガウス分布の最適化"]},{"cell_type":"markdown","metadata":{"id":"mydYwgju-4XN"},"source":["### 3.1 問題設定\n","\n","本セクションで用いるデータは，人工的に生成した2次元混合ガウス分布によるデータです．\n","\n","先ほどの混合ベルヌーイ分布ではデータは784次元でしたが，今回2次元データを用いるのは，各学習ステップにおける分布の変化をわかりやすく可視化するためです．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7vLSaGXXSNyS"},"outputs":[],"source":["# データ分布（混合ガウス分布）を定義してデータを生成する\n","\n","mu0_data = np.array([0, -0.5])\n","sigma0_data = np.array([[1.0, 0], [0, 1.0]])\n","\n","mu1_data = np.array([2.5, 2])\n","sigma1_data = np.array([[0.5, 0.3], [0.3, 0.7]])\n","\n","mu2_data = np.array([-2, 1.5])\n","sigma2_data = np.array([[1.2, 0.2], [0.2, 0.4]])\n","\n","mu_data_list = [mu0_data, mu1_data, mu2_data]\n","sigma_data_list = [sigma0_data, sigma1_data, sigma2_data]\n","sigma_data_inv_list = [np.linalg.inv(sigma) for sigma in sigma_data_list]\n","sigma_data_det_list = [np.linalg.det(sigma) for sigma in sigma_data_list]\n","\n","pi_data_list = [0.45, 0.25, 0.3]\n","\n","\n","NUM_DATA = 3000\n","\n","# 各データをどのガウス分布からサンプリングしてくるか．set {0, 1, 2}\n","categories = np.random.randint(len(pi_data_list), size=NUM_DATA)  # ( 3000, )\n","\n","data_all = []\n","for c in categories:\n","    data_all.append(np.random.multivariate_normal(mu_data_list[c], sigma_data_list[c]))\n","data_all = np.stack(data_all)  # [(2,), (2,).... (2,)] -> ( 3000, 2 )\n","\n","print(f\"data_all: {data_all.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"OZksJBEZjlfF"},"source":["#### 多次元ガウス分布の確率密度関数\n","\n","D次元データに対するガウス分布の確率密度関数は以下で表されます．\n","\n","$$\n","p_{\\mu,\\Sigma}({\\bf x})\n","= \\frac{1}{\\sqrt{(2\\pi)^D|\\Sigma|}}{\\rm exp}\\left\\{-\\frac{1}{2}({\\bf x} - {\\bf \\mu})^T \\Sigma^{-1}({\\bf x} - {\\bf \\mu})\\right\\}\n","\\equiv \\mathcal{N}({\\bf x}; {\\bf \\mu}, \\Sigma)\n","$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mXG0F4qiU5wj"},"outputs":[],"source":["def gaussian_density(\n","    x: np.ndarray, mu: np.ndarray, sigma_inv: np.ndarray, sigma_det: float\n",") -> float:\n","    \"\"\"\n","    多次元ガウス分布の尤度を計算する関数．\n","\n","    Parameters\n","    ----------\n","    x : np.ndarray (2, )\n","        入力データ．\n","    mu : np.ndarray (2, )\n","        ガウス分布のパラメータ（平均値）．\n","    sigma_inv : np.ndarray (2, 2)\n","        ガウス分布のパラメータ（分散共分散行列）．\n","    sigma_det : float\n","        分散共分散行列の行列式．\n","\n","    Returns\n","    -------\n","    p : float\n","        多次元ガウス分布の尤度．\n","    \"\"\"\n","    # x: ( 2, ) mu: ( 2, ) sigma_inv: ( 2, 2 ) sigma_det: ()\n","    diff = x - mu\n","    z = np.exp(-np.dot(diff, np.dot(sigma_inv, diff.T)) / 2)\n","    p = z / np.sqrt((np.power(2 * np.pi, sigma_inv.shape[0]) * sigma_det))\n","    return p"]},{"cell_type":"markdown","metadata":{"id":"Kg8rbChvIPSD"},"source":["混合ガウス分布の周辺分布も，混合ベルヌーイ分布と同じように，ガウス分布の線形和になります．\n","\n","$$\n","p_{{\\bf \\mu},\\Sigma}({\\bf x}) = \\sum_{k=1}^K \\pi_k \\mathcal{N}({\\bf x}; {\\bf \\mu}_k,\\Sigma_k)\n","\\qquad \\sum_{k=1}^K\\pi_k = 1\n","$$\n","\n","以下では，データ生成に用いた混合ガウス分布の確率密度関数と，実際に生成されたデータ点(の一部)を可視化します．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KgXU_f62TYjg"},"outputs":[],"source":["# データ分布とデータ点の可視化\n","NUM_SAMPLES = 300\n","\n","x = np.arange(-5, 5, 0.05)  # x軸\n","y = np.arange(-5, 5, 0.05)  # y軸\n","\n","X, Y = np.meshgrid(x, y)  # ともに ( 200, 200 )\n","\n","Z = np.zeros_like(X)\n","for num in range(len(pi_data_list)):\n","    # すべての格子点についてまとめてdensityを計算するとメモリ不足になる\n","    for i in range(X.shape[0]):\n","        for j in range(Y.shape[1]):\n","            Z[i][j] += pi_data_list[num] * gaussian_density(\n","                np.array([X[i][j], Y[i][j]]),\n","                mu_data_list[num],\n","                sigma_data_inv_list[num],\n","                sigma_data_det_list[num],\n","            )\n","\n","fig, ax = plt.subplots(figsize=(8.0, 6.0))\n","cont10 = ax.contour(X, Y, Z, levels=[0.01 * i for i in range(11)], cmap=\"cool\")\n","\n","data_plot = plt.scatter(\n","    data_all.T[0, :NUM_SAMPLES], data_all.T[1, :NUM_SAMPLES], s=10, c=\"g\"\n",")\n","ax.set_aspect(\"equal\", \"box\")\n","plt.colorbar(cont10)\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"DiEMumMKj9KJ"},"source":["### 3.2 全データ点に対する，混合ガウス分布の対数尤度\n","\n","混合ガウス分布の対数尤度も，混合ベルヌーイ分布の場合と同様に計算でます．\n","\n","データ点の数を$N$，混合数を$K$，ガウス分布を$N({\\bf x}; {\\bf \\mu},\\Sigma)$とすると，全データ点に対する混合ガウス分布の対数尤度は，\n","\n","$$\n","{\\rm log}\\  p_{\\mu,\\Sigma}({\\rm X})= \\sum_{i=1}^N{\\rm log}\\sum_{k=1}^K\\pi_k\\ \\mathcal{N}({\\bf x}_i; {\\bf \\mu}_k, \\Sigma_k)\n","$$\n","\n","で表されます．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nq8Fan4n3Ygi"},"outputs":[],"source":["def log_likelihood(\n","    data_all: np.ndarray, mu_list: np.ndarray, sigma_list: np.ndarray, pi_list: list\n",") -> float:\n","    \"\"\"\n","    全データ点に対する，混合ガウス分布の対数尤度を計算する関数．\n","\n","    Parameters\n","    ----------\n","    data_all : np.ndarray (3000, 2)\n","        すべてのデータ点をまとめた配列．\n","    mu_list : list [ (2), (2), (2) ]\n","        各多次元ガウス分布のパラメータ（平均値）．\n","    sigma_list : list [ (2, 2), (2, 2), (2, 2) ]\n","        各多次元ガウス分布のパラメータ（分散共分散行列）．\n","    pi_list : list (3, )\n","        混合率をまとめたリスト．\n","\n","    Returns\n","    -------\n","    log_likely_all : float\n","        全データ点に対する混合ガウス分布の対数尤度．\n","    \"\"\"\n","    sigma_inv_list = [np.linalg.inv(sigma) for sigma in sigma_list]\n","    sigma_det_list = [np.linalg.det(sigma) for sigma in sigma_list]\n","    log_likely_all = 0\n","    for data in data_all:\n","        likely = 0\n","        for k in range(len(pi_list)):\n","            likely += pi_list[k] * gaussian_density(\n","                data, mu_list[k], sigma_inv_list[k], sigma_det_list[k]\n","            )\n","        log_likely_all += np.log(likely)\n","\n","    return log_likely_all"]},{"cell_type":"markdown","metadata":{"id":"iCsLpdQZkJNJ"},"source":["### 3.3 負担率\n","\n","負担率も混合ベルヌーイ分布の場合と同様に，\n","\n","$i$番目のデータ点に対する，$k$番目の潜在変数$z_{ik}$とすると，負担率は，\n","\n","$$\n","p_{{\\bf \\mu},\\Sigma,\\pi}(z_{ik}=1|{\\bf x}_i)\n","= \\frac{\\pi_k\\ \\mathcal{N}({\\bf x}_i; {\\bf \\mu}_k, \\Sigma_k)}{\\sum_{j=1}^K\\pi_j\\ \\mathcal{N}({\\bf x}_i; {\\bf \\mu}_j, \\Sigma_j)}\n","\\equiv \\gamma(z_{ik})\n","$$\n","\n","で表されます．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d5M6kN0qkJYA"},"outputs":[],"source":["def responsibility(\n","    data_all: np.ndarray, mu_list: list, sigma_list: list, pi_list: list\n",") -> np.ndarray:\n","    \"\"\"\n","    負担率を計算する関数．\n","\n","    Parameters\n","    ----------\n","    data_all : np.ndarray (3000, 2)\n","        すべてのデータ点をまとめた配列．\n","    mu_list : list [ (2), (2), (2) ]\n","        各多次元ガウス分布のパラメータ（平均値）．\n","    sigma_list : list [ (2, 2), (2, 2), (2, 2) ]\n","        各多次元ガウス分布のパラメータ（分散共分散行列）．\n","    pi_list : list (3, )\n","        混合率をまとめたリスト．\n","\n","    Returns\n","    -------\n","    gamma_mat : np.ndarray (3000, 3)\n","        各データの負担率．\n","    \"\"\"\n","    sigma_inv_list = [np.linalg.inv(sigma) for sigma in sigma_list]\n","    sigma_det_list = [np.linalg.det(sigma) for sigma in sigma_list]\n","\n","    gamma_mat = np.zeros((data_all.shape[0], len(pi_list)))  # ( 3000, 3 )\n","    for i in range(gamma_mat.shape[0]):\n","        for k in range(gamma_mat.shape[1]):\n","            gamma_mat[i][k] = pi_list[k] * gaussian_density(\n","                data_all[i], mu_list[k], sigma_inv_list[k], sigma_det_list[k]\n","            )\n","        gamma_mat[i] /= np.sum(gamma_mat[i])\n","    return gamma_mat"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hki0mrpVI9n_"},"outputs":[],"source":["def visualization(\n","    data_all: np.ndarray, mu_list: list, sigma_inv_list: list, sigma_det_list: list\n",") -> None:\n","    \"\"\"\n","    学習時の可視化を行う関数．\n","\n","    Parameters\n","    ----------\n","    data_all : np.ndarray (3000, 2)\n","        すべてのデータ点をまとめた配列．\n","    mu_list : list [ (2), (2), (2) ]\n","        各多次元ガウス分布のパラメータ（平均値）．\n","    sigma_inv_list : list [ (2, 2), (2, 2), (2,2) ]\n","        各多次元ガウス分布の分散共分散行列の逆行列．\n","    sigma_det_list : list [3, ]\n","        各多次元ガウス分布の分散共分散行列の行列式．\n","\n","    Returns\n","    -------\n","    None\n","    \"\"\"\n","    x = np.arange(-5, 5, 0.05)  # x軸\n","    y = np.arange(-5, 5, 0.05)  # y軸\n","\n","    X, Y = np.meshgrid(x, y)\n","\n","    Z = np.zeros_like(X)\n","    for num in range(len(pi_data_list)):\n","        for i in range(X.shape[0]):\n","            for j in range(Y.shape[1]):\n","                Z[i][j] += pi_list[num] * gaussian_density(\n","                    np.array([X[i][j], Y[i][j]]),\n","                    mu_list[num],\n","                    sigma_inv_list[num],\n","                    sigma_det_list[num],\n","                )\n","\n","    fig, ax = plt.subplots(figsize=(6.0, 4.5))\n","    data_plot = ax.scatter(data_all.T[0][0:300], data_all.T[1][0:300], s=7, c=\"g\")\n","\n","    for k in range(len(mu_list)):\n","        ax.scatter(mu_list[k][0], mu_list[k][1], s=60, c=\"r\", marker=\"x\")\n","\n","    ax.set_aspect(\"equal\", \"box\")\n","    cont10 = ax.contour(X, Y, Z, levels=[0.01 * i for i in range(11)], cmap=\"cool\")\n","\n","    plt.colorbar(cont10)\n","    plt.title(\"step:{}\".format(n_iter))\n","    plt.grid(True)\n","\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"_IFTVLUDkdzW"},"source":["### 3.4 学習\n","\n","混合ガウス分布に対しても，EMアルゴリズムの基本的な流れは混合ベルヌーイ分布の場合と同じです．しかし，Mステップにおけるパラメータの更新において，共分散行列の更新も行う必要があります．\n","\n","更新式は以下で表されます（証明等は[こちら](https://qiita.com/kenmatsu4/items/59ea3e5dfa3d4c161efb)などを参照してください）．\n","\n","$$\n","\\hat{\\pi}_k = \\frac{N_k}{N}\n","$$\n","\n","$$\n","\\hat{\\bf \\mu}_k = \\frac{1}{N_k}\\sum_{i=1}^N\\gamma(z_{ik}){\\bf x}_i\n","$$\n","\n","$$\n","\\hat{\\Sigma}_k = \\frac{1}{N_k}\\sum_{i=1}^N\\gamma(z_{ik})({\\bf x}_i - \\hat{\\bf \\mu}_k)({\\bf x}_i - \\hat{\\bf \\mu}_k)^T\n","$$\n","\n","$$\n","ただし，\n","N_k = \\sum_{n=1}^N\\gamma(z_{ik})\n","$$\n","\n","$\\hat{\\pi}_k$は全データの負担率の平均値，$\\hat{\\bf \\mu}_k$は各データ点の負担率による重み付き平均値，$\\hat{\\Sigma}_k$は各データ点の負担率による重み付き共分散に相当します．\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Cj5BUjNep_s"},"outputs":[],"source":["# パラメータ初期化 (適当な値で初期化)\n","mu0 = np.array([-1, -3])\n","sigma0 = np.array([[0.5, 0.1], [0.1, 1.5]])\n","\n","mu1 = np.array([-2.5, 3.2])\n","sigma1 = np.array([[0.7, 0.1], [0.1, 0.5]])\n","\n","mu2 = np.array([1, -1])\n","sigma2 = np.array([[0.8, 0.4], [0.4, 1.0]])\n","\n","mu_list = [mu0, mu1, mu2]\n","sigma_list = [sigma0, sigma1, sigma2]\n","sigma_inv_list = [np.linalg.inv(sigma) for sigma in sigma_list]\n","sigma_det_list = [np.linalg.det(sigma) for sigma in sigma_list]\n","pi_list = [0.3, 0.3, 0.4]\n","\n","\n","n_iter = 0\n","\n","likely = log_likelihood(data_all, mu_list, sigma_list, pi_list) / NUM_DATA\n","print(\"Iteration: {0}, log_likelihood: {1}\".format(n_iter, likely))\n","visualization(data_all, mu_list, sigma_inv_list, sigma_det_list)\n","\n","\n","# 下がり幅がth以下になったら収束したと判定する\n","th = 0.0001\n","\n","# 学習\n","while True:\n","    n_iter += 1\n","\n","    # Eステップ（負担率計算）\n","    gamma_mat = # WRITE ME   # ( 3000, 3 )\n","    n_k = np.sum(gamma_mat, axis=0)\n","\n","    # Mステップ（パラメータ更新）\n","    # piの新しい推定量\n","    pi_list_next = # WRITE ME  # .tolist()\n","\n","    # gamma_mat: ( 3000, 3 ) data_all: ( 3000, 2 ) n_k: ( 3, )\n","    mu_list_next = # WRITE ME  # ( 3, 2 )\n","    # muの新しい推定量\n","    mu_list_next = list(mu_list_next)  # [( 2, ), ( 2, ), ( 2, )]\n","\n","    # Sigmaの新しい推定量\n","    sigma_list_next = []\n","    for k in range(len(pi_list)):\n","        sigma_k = np.zeros_like(sigma_list[k], dtype=float)\n","\n","        for n in range(data_all.shape[0]):\n","            sigma_k += # WRITE ME\n","\n","        sigma_list_next.append(sigma_k / n_k[k])\n","\n","    mu_list = copy.deepcopy(mu_list_next)\n","    sigma_list = copy.deepcopy(sigma_list_next)\n","    pi_list = copy.deepcopy(pi_list_next)\n","\n","    sigma_inv_list = [np.linalg.inv(sigma) for sigma in sigma_list]\n","    sigma_det_list = [np.linalg.det(sigma) for sigma in sigma_list]\n","\n","    likely_before = likely\n","\n","    likely = log_likelihood(data_all, mu_list, sigma_list, pi_list) / NUM_DATA\n","\n","    print(\"Iteration: {0}, log_likelihood: {1}\".format(n_iter, likely))\n","\n","    visualization(data_all, mu_list, sigma_inv_list, sigma_det_list)\n","\n","    delta = likely - likely_before\n","    if delta < th:\n","        break"]},{"cell_type":"markdown","metadata":{"id":"TWo05r1KEUiR"},"source":["## 4. 参考文献\n","[[1]](https://www.jstor.org/stable/2984875) Dempster, A. P., N. M. Laird, and D. B. Rubin. “Maximum Likelihood from Incomplete Data via the EM Algorithm.” Journal of the Royal Statistical Society. Series B (Methodological) 39, no. 1 (1977): 1–38."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EAYKvsL6kk-O"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.1"},"vscode":{"interpreter":{"hash":"1c906a337007ca492b40f9e66323e61f3dcaf71886120485625fb02da1be1aa9"}}},"nbformat":4,"nbformat_minor":0}