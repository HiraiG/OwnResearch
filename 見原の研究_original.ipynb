{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMv2s3y/HwDY+k8zTutCKYx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 見原の研究のオリジナルver\n","---\n","\n","見原の研究のオリジナルverや\n","\n","コメントを追加したverや\n","\n","---\n","最終編集日:2023年6月2日"],"metadata":{"id":"yaqf4cVHyc-U"}},{"cell_type":"markdown","source":["## ライブラリのインポート"],"metadata":{"id":"duo_f5CLzGUI"}},{"cell_type":"code","source":["#モジュールの読み込み\n","from __future__ import print_function\n","\n","import pandas as pd # pandasの読み込み\n","from pandas import Series,DataFrame # なんか知らんけどDataframe変なimportしてる（フツーpd.DataFrame)\n","\n","from sklearn import svm # なんか知らんけどscikit-learn(sklearn)からsvm（サポートベクターマシン）import、ここは無くてもおk\n","from sklearn.model_selection import train_test_split # scikit-learn(sklearn)からtrain_test_splitをimport\n","from sklearn.metrics import accuracy_score # scikit-learn(sklearn)のmetrics関数からaccuracy_scoreをimport\n","from sklearn.preprocessing import StandardScaler # scikit-learn(sklearn)のpreprocessing関数からStandardScalerをimport\n","\n","import numpy as np # numpy（なんぱい）をimporrt\n","import matplotlib.pyplot as plt #matplotlibのpyplot関数をimportしてpltとして定義\n","\n","import tensorflow as tf # kerasのtensorflowをtfとしてimport\n","import seaborn as sns # seaborn（しーぼーん）をsnsとしてimport\n","\n","import time # timeをimport"],"metadata":{"id":"7m5voAMdzKMk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## データの読み込みと分割"],"metadata":{"id":"D7HEplPozMBm"}},{"cell_type":"code","source":["#CSVファイルの読み込み\n","data_set = pd.read_csv(\"general_data.csv\",sep=\",\",header=0)  #pandasのread_csv関数で読み込み\n","\n","#説明変数\n","x = DataFrame(data_set.drop([\"Assignee\"],axis=1)) #pandasのDataFrame関数で\"Assignee\"以外のカラム（列）をxに代入\n","\n","#目的変数\n","y = DataFrame(data_set[\"Assignee\"]) #pandasのDataFrame関数で\"Assignee\"のカラム（列）をxに代入\n","\n","#説明変数・目的変数をそれぞれ訓練データ・テストデータに分割（ランダム），シャッフルしない場合は引数（shuffle=False）\n","x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.5, shuffle=False)"],"metadata":{"id":"gPz3m9przP63"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## データの標準化（または正規化をしよう）\n","---\n","\n","ニューラルネットワークはデータの絶対値が1以下やないと、勾配が爆発して、損失関数lossがNanになる。ほとんどの場合で必須。仮に必須やなくても、こちらの処理をしたほうが実質罰則なし正則化されるので、デメリットなく精度が上がる。やり得。\n","\n","\n","---\n","\n","\n","ただし、決定木（RandomForestやXGBoost、LightGBMなど）は、その仕組み上、標準化はなくても良い\n","\n","---"],"metadata":{"id":"T-wFYY-f1ZpO"}},{"cell_type":"code","source":["# データを標準化\n","stdsc = StandardScaler()\n","x_train = stdsc.fit_transform(x_train)\n","x_test = stdsc.transform(x_test)\n","\n","# 以下すべて変な処理している（フツーしない）\n","\n","#データの整形\n","x_train = x_train.astype(np.float)\n","x_test = x_test.astype(np.float)\n","\n","#カテゴリデータの場合\n","#y_train = keras.utils.to_categorical(y_train,10)\n","#y_test = keras.utils.to_categorical(y_test,10)\n","#数値データの場合\n","y_train = np.array(y_train, dtype = np.float32)\n","y_test = np.array(y_test, dtype = np.float32)"],"metadata":{"id":"1vbGOOg09kSQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## モデルの構築"],"metadata":{"id":"sQa8LZjb9p_G"}},{"cell_type":"markdown","source":["### モデルの定義"],"metadata":{"id":"dN27sR7z_EqI"}},{"cell_type":"code","source":["#ディープニューラルネットワークの実装1 (activation='relu')\n","model = tf.keras.models.Sequential() # モデルの定義\n","\n","t1 = time.time() \n","\n","# 結合層(13ユニット->50ユニット)：入力次元を省略すると自動的に前の層の出力次元数を引き継ぐ\n","# reluやsigmoidがある\n","\"\"\"\n","\n","以下のモデルは、入力から、100, 100, 100, 1のニューラルネット\n","\n","\"\"\"\n","model.add(tf.keras.layers.Dense(100, activation='relu', input_shape=(13,)))\n","model.add(tf.keras.layers.Dropout(0.2)) # ドロップアウトは最悪0でもおk。今回のように大きい（多次元）複雑なデータにはあったほうが良き\n","\n","model.add(tf.keras.layers.Dense(100, activation='relu'))\n","model.add(tf.keras.layers.Dropout(0.2)) #ドロップアウトやバッチ正規化は正則化項なので、複雑なデータを単純化する。最適な値はデータによりけりなので、要調整。\n","\n","model.add(tf.keras.layers.Dense(100, activation='relu'))\n","model.add(tf.keras.layers.Dropout(0.2))\n","\n","#Dropoutを緩めていくと良い（調整の際のコツ）\n","\n","#目的変数がカテゴリデータの場合（10種類の場合）\n","#model.add(Dense(10, activation='softmax'))\n","#目的変数が数値データの場合（出力層．次元1，つまり一つの値を出力する．）\n","model.add(tf.keras.layers.Dense(1))\n","\n","model.summary()\n","print(\"\\n\")"],"metadata":{"id":"8UxBusGJ9soL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### モデルの学習"],"metadata":{"id":"0AuPpojE_IU1"}},{"cell_type":"code","source":["#ここで、どのように学習をするかを定義\n","\n","\n","#ディープニューラルネットワークの実装2\n","model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse'])\n","#勾配法には、'adam'や'sgd'という方法もある\n","#metrics=['accuracy']\n","#metrics=['mae']"],"metadata":{"id":"YunwAzzj_Yn-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#ここで、誤差逆伝播法で学習。\n","#以下はミニバッチを100、エポックを100、学習経過の表示を1エポックごと、教師データを(x_test, y_test)と定義\n","\n","#ディープニューラルネットワークの学習\n","#history = model.fit(x_train, y_train, batch_size=100, epochs=1000, verbose=1, validation_data=(x_test, y_test))\n","history = model.fit(x_train, y_train, batch_size=100, epochs=100, verbose=1, validation_data=(x_test, y_test))"],"metadata":{"id":"5CFgbVTp_ivY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#ここは作成したモデルを使って予測をしている\n","\n","\n","#ディープニューラルネットワークの推論\n","score = model.evaluate(x_test, y_test, verbose=1)\n","print(\"\\n\")\n","print(\"Test Loss:\",score[0])\n","print(\"Test MSE:\",score[1])\n"],"metadata":{"id":"_eEEXD9a_8L_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 可視化（図式化）"],"metadata":{"id":"iF8u6swYAQZa"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"0jbflz00yPqc"},"outputs":[],"source":["# 精度の履歴をプロット\n","# 分類問題のAccuracyは，(True Positive+True Negative)/(True Positive+True Negative+False Positive+False Negative)のようなものを表す．\n","# 回帰の場合は誤差を最小化するのでmseを使うだけで良い．観測値と予測値の差異を数値化したものを表す．\n","plt.plot(history.history['mse'], label=\"Error for Training\", lw=0.5)\n","plt.plot(history.history['val_mse'], label=\"Error for Validation\", lw=0.5)\n","plt.xlabel('Epoch')\n","plt.ylabel('Error')\n","plt.legend(loc='lower center', bbox_to_anchor=(.5, 1.0), ncol=3)\n","plt.savefig(\"error.eps\")\n","plt.close()\n","\n","# 損失の履歴をプロット\n","# 重みパラメータが最適な値に近づくための指標\n","# lossは，学習用データを与えた際の損失値のこと\n","# この値が小さければ小さいほど賢くなったことを表し，\n","# 逆に値が大きければ学習が不十分だということを表す．\n","# val_lossは，検証データを与えた際の損失値．意味は上記のlossと同様．\n","#plt.plot(history.history['loss'], label=\"Loss for Training\", lw=0.5)\n","#plt.plot(history.history['val_loss'], label=\"Loss for Validation\", lw=0.5)\n","#plt.xlabel('Epoch')\n","#plt.ylabel('Loss')\n","#plt.legend(loc='lower center', bbox_to_anchor=(.5, 1.0), ncol=3)\n","#plt.savefig(\"loss.eps\")\n","#plt.close()\n","\n","y_pred = model.predict(x_test)\n","\n","# テストデータに対する推定結果のプロット\n","plt.plot(y_test, lw=0.5)\n","plt.plot(y_pred, lw=0.5)\n","plt.xlabel('Number of Faults')\n","plt.ylabel('Frequency of Assignee')\n","plt.legend(['Testing Data', 'Estimate'], loc='lower center', bbox_to_anchor=(.5, 1.0), ncol=3)\n","plt.savefig(\"frequency.eps\")\n","plt.close()\n","\n","# テストデータに対する推定結果の散布図\n","plt.scatter(y_test, y_pred, c='r', s=10)\n","plt.xlabel('Testing Data')\n","plt.ylabel('Estimate')\n","plt.legend(['Estimate (Frequency of Assignee)'], loc='lower center', bbox_to_anchor=(.5, 1.0), ncol=3)\n","plt.plot([0,10],[0,10])\n","plt.savefig(\"general_scatter.eps\")\n","plt.close()\n","\n","# テストデータに対する推定された累積時間データのプロット\n","plt.plot(np.cumsum(y_test), lw=0.5)\n","plt.plot(np.cumsum(y_pred), lw=0.5)\n","plt.xlabel('Number of Faults')\n","plt.ylabel('Cumulative Frequency of Assignee')\n","plt.legend(['Testing Data', 'Estimate'], loc='lower center', bbox_to_anchor=(.5, 1.0), ncol=3)\n","plt.savefig(\"cfrequency.eps\")\n","plt.close()\n","\n","# テストデータに対する累積時間データの散布図\n","plt.scatter(np.cumsum(y_test), np.cumsum(y_pred), c='r', s=10)\n","plt.xlabel('Testing Data')\n","plt.ylabel('Estimate')\n","plt.legend(['Estimate (Cumulative Frequency of Assignee)'], loc='lower center', bbox_to_anchor=(.5, 1.0), ncol=3)\n","plt.plot([0,1000],[0,1000])\n","plt.savefig(\"cscatter.eps\")\n","plt.close()\n","\n","t2 = time.time()\n"," \n","# 経過時間を表示\n","elapsed_time = t2-t1\n","print(f\"経過時間：{elapsed_time}\")\n"]}]}